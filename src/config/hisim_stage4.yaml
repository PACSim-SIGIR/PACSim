runner: "episode_runner"
mac: "basic_mac"
learner: "q_learner"

# Stage4: online RL (policyâ†’LLM decoupling), no external LLM by default
ddp_find_unused_params: true

# Optimizers use belief_net_lr/encoder_lr/mixer_lr (not train.learning_rate)
use_mixer: false
lr: 0.0003
belief_net_lr: 0.0001
encoder_lr: 0.0
mixer_lr: 0.0
weight_decay: 0.0
lambda_sd: 0.0
lambda_m: 0.0

action_selector: "multinomial"
multinomial_sample_mode: "softmax"
action_temperature: 1.8
epsilon_start: 0.35
epsilon_finish: 0.15
epsilon_anneal_time: 200000
test_greedy: true

t_max: 500000
test_interval: 2000
test_nepisode: 200

n_agents: 300
belief_dim: 128
population_belief_dim: 3
text_embed_dim: 1024
batch_size_run: 1
n_actions: 5
max_seq_length: 1024

# prompt_embedding_dim is fixed at 2, so prompt_attention_heads must divide 2
prompt_attention_heads: 2
commitment_embedding_dim: 1024

arch:
  entity_dim: 256
  attention_heads: 4
  transformer_blocks: 2
  key_dim: 64
  mlp_hidden_size: 256
  feedforward_size: 1024
  dropout_rate: 0.1
  layer_norm_epsilon: 0.00001

env_action_source: "sync_stage_policy"
enable_llm_rollout: false
together_api_key: "${TOGETHER_API_KEY}"
# llm_model_name is only for tokenizer
llm_model_name: "gpt2"
coordinator_model: "gpt-3.5-turbo"
executor_model: "gpt-3.5-turbo"
llm_response_format_json: true
use_nonparam_group_repr: true
population_update_use_group_repr: true
population_update_use_stage: true
use_brief_encoder: false
brief_encoder_input_dim: 128
brief_encoder_hidden_dim: 256
brief_encoder_use_stage: false
population_update_parametrization: "dirichlet"
dirichlet_alpha_min: 0.001

env: "hisim_social_env"
env_args:
  hisim_data_root: "${HISIM_DATA_ROOT}"
  topic: "metoo"
  event: "e1"
  max_question_length: 1024
  max_answer_length: 512
  label2id_path: "./assets/label2id/metoo_e1.json"

# env: "hisim_social_env"
# env_args:
#   hisim_data_root: "${HISIM_DATA_ROOT}"
#   topic: "blm"
#   event: "p1"
#   max_question_length: 1024
#   max_answer_length: 512
#   label2id_path: "./assets/label2id/blm_p1.json"

  n_stages: 10
  sync_stage_update: true
  expected_core_users: 300
  max_core_users: 300
  shuffle_core_users_each_stage: true
  core_users_shuffle_seed: 42
  max_neighbor_posts: 8
  max_population_texts: 20
  max_user_history_lines: 40
  max_recent_self_posts: 6
  group_representation_dim: 128

  use_secondary_belief_sim: true
  secondary_sim_max_users: 700
  secondary_sim_use_micro_texts: true
  population_belief_mode: "categorical3"
  # When population_z_updater="noop", env uses secondary_z_next from policy/belief_encoder
  population_z_updater: "noop"

  reward_w_action_type: 0.0
  reward_w_stance: 0.0
  reward_w_text: 0.0
  reward_w_z: 1.0
  reward_z_on_stage_end_only: true
  mask_missing_gt: true
  min_edge_labels_for_z_target: 20
  soft_z_mask: true

# Optional: inject S3b prior bias into RL logits
s3b_bias_enabled: true
s3b_bias_mode: "diff01"
s3b_bias_alpha: 0.5
s3b_bias_alpha_trainable: false
s3b_bias_alpha_max: 0.8
s3b_bias_gate_by_stance: true
s3b_bias_stance_min_conf: 0.95

curriculum:
  enabled: true
  t_env_steps: [20000, 60000]
  n_stages: [7, 10, 13]

z_transition_loss_weight: 0.0
z_transition_loss_type: "kl"

# Freeze representation modules during RL (default for S4)
freeze_belief_encoder_in_rl: true
freeze_belief_network_in_rl: true
freeze_stance_head_in_rl: false
freeze_action_type_head_in_rl: false
freeze_mixer_in_rl: false

train_policy_heads_in_rl: true
train_stance_head_in_rl: true

train:
  episodes_per_task: 1
  buffer_size: 128
  batch_size: 32
  update_interval: 5
  optimizer: "adam"
  learning_rate: 0.001
  coordinator_learning_rate: 0.0005
  gamma: 0.99

system:
  use_cuda: true
  device_num: 0
  seed: 42
  debug: true

logging:
  use_tensorboard: true
  log_interval: 1
  save_model: true
  save_model_interval: 2000
  checkpoint_path: "./models"
  log_path: "./logs"
  experiment_name: "hisim-social-stage4-zreward"

