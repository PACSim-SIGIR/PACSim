runner: "episode_runner"
mac: "basic_mac"
learner: "q_learner"

# Stage3a: offline z(t)->z(t+1) transition supervision (K=3)
t_max: 50000
test_interval: 5000
test_nepisode: 256

n_agents: 3
belief_dim: 128
population_belief_dim: 3
batch_size_run: 1

n_actions: 5
env_action_source: "discrete_action_boxed"

enable_llm_rollout: false
together_api_key: ""
coordinator_model: "gpt-3.5-turbo"
executor_model: "gpt-3.5-turbo"
llm_model_name: "gpt2"

lr: 0.001
encoder_lr: 0.001
belief_net_lr: 0.0
mixer_lr: 0.0
weight_decay: 0.0
lambda_sd: 0.0
lambda_m: 0.0
stage2_weight: 0.0
bne_max_iterations: 0

arch:
  entity_dim: 256
  attention_heads: 4
  transformer_blocks: 2
  key_dim: 64
  mlp_hidden_size: 256
  feedforward_size: 1024
  dropout_rate: 0.1
  layer_norm_epsilon: 0.00001

prompt_attention_heads: 2
commitment_embedding_dim: 1024

train_encoder_only: true
use_mixer: false

# Stage3a: train population_update_head (+ brief encoder), keep policy fixed
population_update_use_stage: true
train_population_update_head_only: true
use_nonparam_group_repr: true
population_update_use_group_repr: true

use_brief_encoder: true
brief_encoder_input_dim: 128
brief_encoder_hidden_dim: 256
brief_encoder_use_stage: false
train_brief_encoder_in_stage3a: true

z_transition_loss_weight: 1.0
# Dirichlet head; rollout uses E[z]; loss uses Dirichlet KL
population_update_parametrization: "dirichlet"
dirichlet_alpha_min: 0.001
dirichlet_alpha0_target: 10.0
z_transition_loss_type: "dirichlet_kl"

env: "huggingface_dataset_env"
env_args:
  hf_dataset_path: 
    - "./data/stage_3a_dataset_metoo_e1_z_transition"
    - "./data/stage_3a_dataset_metoo_e2_z_transition"
  dataset_split: "train"
  question_field_name: "question"
  answer_field_name: "answer"
  max_question_length: 1024
  max_answer_length: 128
  dataset_streaming: false
  use_random_sampling: true
  use_dataset_episode: false
  filter_is_core_user: null
  n_actions: 1
  emit_belief_fields: true
  population_belief_dim: 3
  infer_dirichlet_alpha0_from_z_target: true
  infer_dirichlet_alpha0_max_den: 2000
  verbose_step_logging: false


reward:
  al_weight: 0.0
  ts_weight: 0.0
  cc_weight: 0.0

train:
  buffer_size: 256
  batch_size: 64
  update_interval: 10
  optimizer: "adam"
  learning_rate: 0.001
  coordinator_learning_rate: 0.0005
  gamma: 0.99

system:
  use_cuda: true
  device_num: 0
  seed: 42
  debug: false

logging:
  use_tensorboard: true
  log_interval: 50
  save_model: true
  save_model_interval: 5000
  checkpoint_path: "./models"
  log_path: "./logs"
  experiment_name: "hisim-z-transition-s3a-e"

